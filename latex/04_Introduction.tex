\chapter{Introduction}
\label{chap:intro}

\section{Optimal Control and Learning}
\label{chap:introOptimalControlLearning}
Control design using mathematical optimization has found numerous applications in science and engineering (e.g. aerospace vehicles, robotics, chemical plants), economics (e.g. markets and investment strategies), and operations research (e.g. supply and/or inventory optimization).  Generally, an optimal control solution is a set of coupled differential (or difference) equations describing a system trajectory optimizing a specified objective.  The objective may contain terminal constraints and integrated cost along the solution trajectory.  Optimal control problems are typically solved via Pontryagin's maximum principle or Bellman's optimality equations using dynamic programming (DP) \cite{stengel,kirk} or, in some cases where the plant is linear, time-invariant and of ``low'' dimension, closed-form (analytic) solutions may be obtained.

The origins of optimal control theory date back as early as late 1600s, when the ``brachistochrone problem'' was first solved.  The problem dealt with finding the control function or feedback which optimizes a performance index involving differential equation constraints.  In the 1700s, work by Euler and Lagrange resulted in a new theory, the calculus of variations, which (in modern terminology) involved multipliers that captured sensitivities of the performance index to changes in states.  Further work by Legendre, Jacobi, and Hamilton related to sufficient conditions for optimality, resulting in a theory forming the basis of the modern dynamic programming algorithm.  Weierstrass took these results further to develop theory which was fundamental to the ``maximum principle'' developed by Pontryagin and ``principle of optimality'' developed by Bellman for dynamic programming \cite{bryson1996optimal}.

In the 1960's, Kalman and others introduced a new quadratic, integrated performance index for linear systems which provided the controls as a linear function of the states \cite{kalman1960contributions}.  This new linear quadratic (LQ) theory applied to time-varying systems and multi-input-multi-output systems, and represented a significant shift toward automated approaches to aerospace problems involving trade-offs between control effort and performance.  The LQ-optimal control can be parameterized analytically using Riccati equations, assuming full-state feedback and a perfect state model, and provides a theoretically guaranteed gain margin of 6 dB and phase margin of 60 deg \cite{safonov1977gain}.  However, this theoretical result assumes the feedback control has access to perfect observations (measurements) of the full internal state, that the system input has infinite bandwidth and control inputs are not clipped.

It is also interesting to note that optimal control and reinforcement learning (RL) are closely related.  RL computes the optimal control using only observations (or estimates) of the objective function, and has been likened to a ``direct'' \cite{slotineli} form of adaptive optimal control \cite{sutton1992reinforcement} based on approximate dynamic programming (ADP).  Actor-critic techniques, policy gradient techniques, and Q-Learning all fall under this broad umbrella and provide a mechanism by which the state-space is ``explored'' in order to optimize (``exploit'') the resulting observations of the objective function.  RL's trade-off between exploration and exploitation is strikingly similar to the idea of ``persistent excitation'' in system identification.  A common benchmark in both RL and optimal control is the LQ problem, since its solution is known analytically.

\section{Sensitivity and Robust Control}
\label{chap:introRobustnessAnalysis}
It is important to note that no real-world system is perfectly modeled, fully characterized by its measurements, or even linear.  For example, higher-frequency disturbances may occur at the plant's input (e.g. actuator noise, wind gusts for aircraft, currents for marine vehicles) or output (e.g. sensor measurement noise).  Lower-frequency dynamic perturbations may be induced by discrepancies between the plant model and true system (e.g. actuator dynamics, nonlinearities, parameter variation due to environmental effects).  Most reduced-order models become increasingly inaccurate at higher frequencies.

Additionally, for partially-observable systems requiring state estimation, theoretical guarantees of closed-loop robustness are no longer possible.  In the LQ context, as pointed out in Doyle's seminal paper on LQ Gaussian (LQG) control \cite{doyle1978guaranteed}, there exist plants and cost weights for which the closed-loop system could be arbitrarily fragile.  The specific example provided highlights the effect of a Kalman filter which assumes an LQ controller (based on a nominal model), thus making the loop transfer function highly sensitive to modeling error in a manner dependent on user-selected cost weights.  By penalizing state error more strictly, the LQG closed-loop system effectively relies too heavily on model predictions.

In any case, it is very important that a feedback system be verified and tested to be robust to uncertainty and/or disturbances.  Within control theory, sensitivity is a fundamental area with a rich set of results spanning decades.  The full scope extends beyond the limited scope of this thesis, but a brief summary of relevant theory (e.g., loop transfer functions, sensitivity functions, integral constraints) can be found in Appendix \ref{appendix:sensitivity}.

It suffices to note there are known results which constrain the allowable shape of various closed-loop transfer functions in the frequency domain, including their peak values (i.e., $\mathcal{H}_{\infty}$ norms) according to intrinsic properties of loop transfer functions.  Such constraints provide a general framework for deriving robustness bounds.  For example, sensitivity trade-offs are fundamental to loop-shaping control designs, and explain why improperly-shaped loops which ignore physical limitations such as bandwidth limitations, delays, or input saturation can cause latent instabilities \cite{stein2003respect}.  Robustness despite delays and/or unstable poles/zeros has been used as a surrogate for understanding the fragility of complex feedback networks seen in other applications such as biology, neuroscience, ecology, or multi-scale physics \cite{doyle2011universal, leong2016understanding, doyle2017universal}.

Compared to the single-input single output (SISO) case, the design of robust multi-input multi-output (MIMO) controllers is considerably more complicated (both theoretically and computationally) as the plant gain, zeros/poles, delays, and disturbances all have an associated ``direction'' which makes it more difficult to separate their effects.  Typically, MIMO loop transfer functions can be satisfactorily described using singular values, an approach this thesis will also adopt.  The plant to be controlled will be assumed to have a discrete-time state-space realization, which will be generalized into a form which allows the response from disturbances to performance to be characterized easily using singular values.

By contrast, robustness in reinforcement learning has received much less attention.  Deep learning approaches which rely on overfitting can struggle with robustness when deployed to an environment significantly different from their training environment (not unlike the LQG example of relying too heavily on a nominal model).  Often times, RL operates on a probabilistic representation of the state dynamics and uncertainty.  Naively assuming that more data alone leads to optimal performance misses the fact that many real-world uncertainties are not zero-mean additive noise.

Multi-agent reinforcement learning and min-max dynamic games seem to offer a more meaningful framework to explore the robustness of learned feedback systems.  It is well-known, e.g. by \cite{basar1989dynamic, rhee1991game, lee1990interconnections, bernhard1991lecture, bacsar2008h}, that the two-player zero-sum dynamic game can be written as an $\mathcal{H}_{\infty}$ optimization and is therefore feasibly able to address parameter uncertainty or modeling error.  The important aspect (which is shared with robust control theory) is that the fundamental trade-off between optimality and robustness is captured by the performance function being optimized.  The extent to which traditional robust control can be applied to machine learning and model-free feedback problems is an interesting and open research area.

\section{Modeling of Uncertainty}
\label{chap:introModelingUncertainty}
Another challenging aspect of robustness is that disturbances or uncertainty are often unique to the application.  While robust control can provide semi-automated techniques for analyzing the effect of disturbances on performance, the results are valid in a very generalized sense.  The best results are typically obtained when a known uncertainty structure can be specified based on insight into the system.

For instance, one (possibly oversimplified) approach lumps all dynamic uncertainties into a single perturbation block, $\Delta$, which for LTI systems is interpreted as an unknown transfer function.  This \emph{unstructured} dynamic uncertainty can then be tailored to describe various scenarios, e.g., additive, inverse additive, input/output multiplicative, inverse input/output multiplicative, or coprime-factor perturbations.  Unstructured uncertainty is well-suited to describe unknown or un-modeled dynamics; in particular, additive uncertainty can roughly approximate ``absolute error'' between nominal and actual dynamics, and multiplicative uncertainty can roughly approximate ``relative error''.  Often these can occur at higher frequencies, e.g., due to delays, hysteresis, or nonlinearities \cite{gu2005robust}.

Another category, \emph{parametric} uncertainty, occurs as a result of inaccurate component specifications, or as a result of system degradation.  In this case, known parameters are subject to low- or zero-frequency changes within some possible range of values.  Parametric uncertainty implies a particular known structure, which (depending on the plant modeling approach) may not affect all of the states.  Thus, an unstructured uncertainty description will often result in overly conservative feedback designs when compared to a design which leverages plant structure known \emph{a priori}.  Parametric uncertainty is a specific case of structured uncertainty, typically described by letting the plant take a generalized linear fractional representation (LFR).  This allows a more general mathematical treatment about how uncertainty and/or controlled inputs affect the input/output relationship of the feedback system as a whole \cite{gu2005robust}.

\section{Data-Driven Control}
\label{chap:introDataDrivenControl}
Data-driven control is a broad term referring to algorithms in which model identification or controller design is based entirely on experimental (``training'') data gathered from the system to be controlled.  This is common within engineering disciplines, even when a first principle model is available, often to fit specific values of parameters using controlled experiments.  In aerospace and marine vehicle designs, wind tunnels and tow tanks using scale models are used to gather data which can help approximate (often highly nonlinear) dynamics using reduced-order models.

Typically, system identification is best carried out for stable systems with a relatively low number of dominant modes, which can be identified from the input-output data of controlled experiments.  Popular techniques for linear model identification include prediction error minimization (PEM) or subspace identification (SID), and in general these approaches are performed in an offline, batched manner.  Algorithms which identify a model from data and use it for control design are referred to as \emph{indirect} techniques.

By contrast, \emph{direct} techniques do not explicitly identify a plant model, and instead design the controller or policy directly from experimental data.  (In control theory this might be called direct adaptive control.)  Notable techniques include reinforcement learning and recently, convex optimization.  The benefit of direct data-driven control is that a model is not needed, and the system identification problem can be omitted.

For non-engineering applications in which the data gathered may come from an imprecisely characterized system, this becomes more important.  Some common system identification methods only guarantee accuracy with infinite data, which is impossible.  Some direct data-driven techniques thus aim for ``finite-horizon guarantees'' of stability, robustness, or optimality.  The amount of data required to learn the optimal solution is referred to as the ``sample complexity'' \cite{jedra2019sample, dean2020sample, zhang2021derivative}.  For online learning or adaptive control, algorithms must have a low sample complexity in order to be applied to real problems.

\section{Motivation for Study}
\label{chap:introMotivationForStudy}
It is not difficult to make the connection between data-driven control and machine learning.  For controlled systems, optimal performance is obtained by incorporating analytical reasoning of physical constraints into the decision making process.  If precise mathematical models of the system are available, control inputs / signals may be automatically calculated to achieve desired system behavior, specified in terms of some cost or value function to optimize.  This idea is central to optimal control and reinforcement learning in particular.

However, a system must also be able to accomplish desired tasks despite uncertainty.  While robustness of linear plants is a well-studied topic in control theory, it is still assumed that a reasonably accurate model of the uncertainty is available in order to fully achieve non-conservative designs.  For systems with imprecise mathematical models, it is difficult to apply control techniques as one may not know how uncertainty affects dynamics.

While data-driven techniques have potential to address some of these challenges, robustness guarantees in model-free learning are often only provable under asymptotic or averaged contexts.  More work is required to fully understand the role that robustness plays in machine learning, even for relatively simple systems.  As autonomous systems are designed which may incorporate optimization-based learning within their decision making processes, it is crucial to understand the relative merits of system identification, model-based design, and model-free design for system robustness for a particular application.

\section{Literature Review}
\label{chap:introLiteratureReview}
The connection between LQ regulation and reinforcement learning remains a popular topic in academic contexts.  Foundations laid by Bradke and Barto \cite{bradtke1992reinforcement, bradtke1994adaptive} have given way to a variety of similar techniques involving policy/value iteration \cite{lamperski2020computing, yaghmaie2022linear, lewis2009adaptive, lewis2009reinforcement, farjadnasab2022model, wong2010reinforcement, lale2021adaptive, chen2019adaptive, cui2021combined, matni2019self, yaghmaie2019using, yang2021model, rizvi2018output, cohen2018online}, policy optimization \cite{malik2019derivative} and certainty-equivalent model-based design \cite{dean2020sample}.  Robustness in RL is another topic which has been investigated \cite{moos2022robust, venkataraman2019recovering, roberts2011feedback, pang2022robust, umenberger2019robust, dean2018regret, ho2019robust, bernat2020driver, al2007model, al2007model2, zhang2021derivative, zhang2020policy}.

Robust control refers to a set of theoretical results developed during the 1980s-1990s following the realization that, depending on the plant, LQG control could have vanishingly small stability margins \cite{doyle1978guaranteed}.  A high-level chronological overview given by Safonov \cite{safonov2012origins}.  Safonov, Laub \& Hartmann \cite{safonov1981feedback} showed that for multivariable systems, a fundamental performance vs. robustness trade-off can be quantified using Bode magnitude vs. frequency plots of singular values of the return difference matrix.  Additionally, the stability margins of multi-loop systems could be characterized in terms of convex, frequency-dependent sets of modeling uncertainty \cite{safonov1981multiloop}.  Connections to classical open-loop Bode gain plots were provided by Doyle and Stein \cite{doyle1981multivariable}, and further development of frequency-domain approaches led to automated synthesis procedures based on optimization of the sensitivity and complementary sensitivity functions, based on the work of Zames on $\mathcal{H}_{\infty}$ control \cite{zames1981feedback}.

Corresponding state-space solutions to $\mathcal{H}_{\infty}$ and $\mathcal{H}_{2}$ (a.k.a. LQG) control problems were given by Doyle \cite{doyle1988state}, along with an even more generalized linear fractional transformation (LFT) interpretation \cite{doyle1991review} allowing for convex optimization using linear matrix inequalities (LMIs).  Other references to state-space robust control are \cite{limebeer1989discrete, iglesias1991state, toivonen1995lecture, deodhare1998modern}, along with game-theoretic interpretations by Basar and Bernard \cite{basar1989dynamic, bacsar2008h, bernhard1991lecture} and Speyer \cite{rhee1991game}.

It has also been observed that since $\mathcal{H}_{\infty}$ control solutions are non-unique, additional constraints can be added to the optimization problem, namely the incorporation of mixed-norm $\mathcal{H}_{2}$/$\mathcal{H}_{\infty}$ designs which seek to minimize a weighted combination of both objectives.  This can be done using one or multiple performance outputs (e.g. producing two transfer functions, one whose $\mathcal{H}_{2}$ norm is minimized, the other whose $\mathcal{H}_{\infty}$ norm is minimized).  Such techniques have been explored in several papers by Bernstein and Mustafa \cite{haddad1990generalized, haddad1991mixed, mustafa1991lqg} as a potential way of enhancing robustness of LQG controllers.
%
% Youla: \cite{kuvcera2007h2,mahtout2020advances}
%
% Performance limits: \cite{stein2003respect, freudenberg1985right}
%
% Assumption-free robust control: safonov2018robust
%
% Disk margins: seiler2020introduction
%

In many cases, model-based synthesis of robust controllers involves the solution of an algebraic Riccati equation (ARE), which has a variety of solution techniques.  Nonrecursive approaches based on decomposition of the system Hamiltonian (in continuous-time) or symplectic (in discrete-time) matrix are provided by Vaughan's nonrecursive negative exponential \cite{vaughan1969negative} and eigendecomposition \cite{vaughan1970nonrecursive} techniques, respectively.  In the latter (discrete-time) case, Schur decomposition is considered to be preferable from a numerical point of view \cite{laub1979schur}.  Additional discussion on numerical recipes for AREs can be found in \cite{pappas1980numerical, gardiner1986generalization, gudmundsson1992scaling, chen1994non, takaba1996discrete, feng2009solving, rojas2011discrete, aliev1992discrete}.

Data-driven control can be broadly categorized into indirect approaches (relying on model identification from data) and direct approaches (involving no explicit state model).  Early results involving linear system identification include \cite{ho1966effective, juang1985eigensystem, ljung1987theory, smith1989model, bayard1992criterion, schrama1992accurate, makila1995worst, mckelvey1996subspace, de1997suboptimal, ljung1998system}.  Modern data-driven control methods often cite Willems' fundamental lemma \cite{willems2005note} which proved that for linear systems, an input trajectory of sufficiently high order can generate state trajectories amenable to both system identification and model-free optimization.

Examples of indirect data-driven approaches include \cite{qin2006overview, di2021confidence, shin2020unifying, proctor2016dmdc, lu2020characterizing, zhang2019online, leyang2012properties}, and examples of direct data-driven approaches include \cite{de2019formulas, berberich2020robust, van2020noisy, berberich2022combining}.

%Closely-related work in robust optimization includes \cite{soyster1973convex, ei1997robust, el1998robust, ben1999robust, ben2001lectures, ben2002robust, bertsimas2003robust, bertsimas2004price, chaerani2006modeling, joelianto5417249}.
