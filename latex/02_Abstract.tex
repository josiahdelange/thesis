\chap{Abstract}
\label{chap:abstract}
\noindent Machine learning and optimization are becoming increasingly prevalent in many areas of science, including statistics, robotics, and artificial intelligence.  A growing area of research is the incorporation of feedback/reinforcement into learning techniques, to realize adaptive control and decision-making without full information on all factors which may affect performance.  Although many results in model-free reinforcement learning are promising, their use in real-world applications has been subject to criticism, and in many cases, considered too fragile for tasks where failure is expensive and potentially catastrophic. As new learning techniques are developed, it is important to understand the relative merits of data-driven control, system identification and model-based control.  This thesis focuses on recent advances in offline data-driven control based on convex optimization, with the key objective of investigating the robustness of static state feedback gains learned from data.  For several case study systems, static LQR/$\mathcal{H}_{2}$ and $\mathcal{H}_{\infty}$ regulators will be synthesized, comparing a \emph{direct} data-driven approach and \emph{indirect} combined system identification/model-based approach with a \emph{baseline} model-based synthesis approach.  Test scenarios considered here include variations in the measurement noise and a plant parameter affecting its inertial properties.  Singular values, disk margins, and integrated cost metrics will be used to quantify the results.\\

\noindent \textbf{Primary Reader and Advisor:}~Dr. Neil F. Palumbo \\
\textbf{Secondary Reader:}~Dr. Cleon Davis \\
\textbf{Tertiary Reader:}~Mr. Gregg Harrison\\